{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-learn pandas tabulate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "target_names = newsgroups['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'documents': newsgroups['data'],\n",
    "    # 'target_names': newsgroups['target_names'],\n",
    "    'target': [ target_names[_] for _ in newsgroups['target'] ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(20)\n",
    "\n",
    "emails = df['documents'].to_list()\n",
    "shuffled_emails = random.sample(emails, len(emails))[:20]\n",
    "\n",
    "# print(df.iloc[1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def cosine(vector1: List[float], vector2: List[float]) -> float:\n",
    "    if len(vector1) != len(vector2):\n",
    "        raise Exception(\"Both vector length should be same\")\n",
    "\n",
    "    dot_product_vector = [ vector1[i] * vector2[i] for i in range(len(vector1)) ]\n",
    "    dot_product = sum(dot_product_vector)\n",
    "\n",
    "    vector1_abs = sqrt(sum([ val**2 for val in vector1 ]))\n",
    "    vector2_abs = sqrt(sum([ val**2 for val in vector2 ]))\n",
    "\n",
    "    return (dot_product / (vector1_abs * vector2_abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import re\n",
    "from typing import Dict, List\n",
    "from collections import Counter\n",
    "from tabulate import tabulate\n",
    "\n",
    "NON_WORDS = re.compile(\"[^a-z' ]\")\n",
    "\n",
    "class Corpus:\n",
    "\n",
    "    def __init__(self, corpus: List[str]):\n",
    "        self.raw_docs = corpus\n",
    "        self.corpus = [ NON_WORDS.sub(' ', document.lower()).split() for document in corpus ]\n",
    "        self.document_set = [ set(document) for document in self.corpus ]\n",
    "        self._fit()\n",
    "        self.vector_space = [ \n",
    "           self._transform(document) for document in self.corpus\n",
    "        ]\n",
    "\n",
    "    def get_scores(self, words: List[str]):\n",
    "        count_dict = Counter(words)\n",
    "\n",
    "        word_list = []\n",
    "        tf = []\n",
    "        df = []\n",
    "        idf = []\n",
    "        freq = []\n",
    "        tf_idf = []\n",
    "\n",
    "        for word, value in count_dict.items():\n",
    "            word_list.append(word)\n",
    "\n",
    "            _tf = value / len(count_dict.items())\n",
    "            _df = len([ document for document in self.document_set if word in document ])\n",
    "            _idf = log((1 + len(self.corpus)) /  (1 + _df))\n",
    "\n",
    "            tf.append(_tf)\n",
    "            idf.append(_idf)\n",
    "            freq.append(value)\n",
    "            tf_idf.append(_tf * _idf)\n",
    "            df.append(_df)\n",
    "\n",
    "        return {\n",
    "            'words': word_list,\n",
    "            'freq': freq,\n",
    "            'tf': tf,\n",
    "            'idf': idf,\n",
    "            'tf-idf': tf_idf,\n",
    "            'df': df\n",
    "        }\n",
    "    \n",
    "    def get_document_scores(self, document_id: int):\n",
    "        return self.get_scores(self.corpus[document_id])\n",
    "    \n",
    "    def show_df(self, table: Dict[str, List[float]], limit: int = 3):\n",
    "        table_data = list(zip(table['words'], table['freq'], table['tf'], table['idf'], table['tf-idf'], table['df']))\n",
    "\n",
    "        # Sort by TF*IDF in descending order\n",
    "        table_data.sort(key=lambda x: x[4], reverse=True)\n",
    "\n",
    "        print(tabulate(table_data[:limit], headers=[\"Word\", \"Frequency\", \"TF\", \"IDF\", \"TF*IDF\", \"DF\"], tablefmt=\"pretty\"))\n",
    "\n",
    "    def _fit(self):\n",
    "        vocabulary = set()\n",
    "        for document in self.document_set:\n",
    "            vocabulary.update(document)\n",
    "        \n",
    "        vocabulary = list(vocabulary)\n",
    "        self.vocabulary_ = {}\n",
    "        for i in range(len(vocabulary)):\n",
    "            self.vocabulary_[vocabulary[i]] = i\n",
    "\n",
    "    def _transform(self, query_words: List[str]) -> List[float]:\n",
    "        if not hasattr(self, 'vocabulary_'):\n",
    "            raise Exception(\"Please call _fit() method first\")\n",
    "        \n",
    "        query_scores = self.get_scores(query_words)\n",
    "        word_tf_idf_tuple = list(zip(query_scores['words'], query_scores['tf-idf']))\n",
    "        word_to_tf_idf_mapping = {}\n",
    "        for pair in word_tf_idf_tuple:\n",
    "            word_to_tf_idf_mapping[pair[0]] = pair[1]\n",
    "\n",
    "        query_vector = [0.0] * len(self.vocabulary_)\n",
    "\n",
    "        for word in query_words:\n",
    "            index = self.vocabulary_.get(word)\n",
    "            \n",
    "            if not index:\n",
    "                continue\n",
    "            \n",
    "            query_vector[index] = word_to_tf_idf_mapping[word]\n",
    "\n",
    "        return query_vector\n",
    "    \n",
    "    def transform(self, query: str) -> List[float]:\n",
    "        query_words = NON_WORDS.sub(' ', query.lower()).split()\n",
    "        return self._transform(query_words)\n",
    "    \n",
    "    def search(self, query: str, limit: int = 3) -> List[tuple[str, float]]:\n",
    "        query_vector = self.transform(query)\n",
    "\n",
    "        rankings = []\n",
    "        for i in range(len(self.vector_space)):\n",
    "            vector = self.vector_space[i]\n",
    "            cosine_rank = cosine(query_vector, vector)\n",
    "            rankings.append((self.raw_docs[i], cosine_rank))\n",
    "\n",
    "        rankings.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return rankings[:limit]\n",
    "\n",
    "\n",
    "corpus = Corpus(shuffled_emails)\n",
    "table = corpus.get_document_scores(4)\n",
    "# corpus.show_df(table, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"From: franjion@spot.Colorado.EDU (John Franjione)\\nSubject: Re: Jack Morris\\nNntp-Posting-Host: spot.colorado.edu\\nOrganization: University of Colorado, Boulder\\nLines: 15\\n\\ntedward@cs.cornell.edu (Edward [Ted] Fischer) writes:\\n\\n>-Valentine\\n>(No, I'm not going to be cordial.  Roger Maynard is a complete and\\n>total dickhead.  Send me e-mail if you insist on details.)\\n\\nIn fact, he's a complete and total dickhead on at least 2 newsgroups\\n(this one and rec.sport.hockey).  Since hockey season is almost over,\\nhe's back to being a dickhead in r.s.bb.\\n\\n-- \\nJohn Franjione\\nDepartment of Chemical Engineering\\nUniversity of Colorado, Boulder\\nfranjion@spot.colorado.edu\\n\",\n",
       "  0.2277065800916448),\n",
       " (\"From: S_BRAUN@IRAV19.ira.uka.de (Thomas Braun)\\nSubject: sources for shading wanted\\nOrganization: University of Karlsruhe, FRG\\nLines: 22\\nDistribution: world\\nNNTP-Posting-Host: irav19.ira.uka.de\\nX-News-Reader: VMS NEWS 1.25\\n\\nI'm looking for shading methods and algorithms.\\nPlease let me know if you know where to get source codes for that.\\n\\nThanks a lot!\\n\\nThomas\\n\\n\\n+-----------------------------------------------------------------------------+\\n|                   Thomas Braun, Universitaet Karlsruhe                      |\\n|                    E-Mail : S_BRAUN@iravcl.ira.uka.de                       |\\n+-----------------------------------------------------------------------------+\\n \\n\\n+----------------------------------------------------------------------------+\\n|  \\\\_\\\\_\\\\_\\\\_\\\\_ \\\\_\\\\_\\\\_     Thomas Braun                                        |\\n|       \\\\_     \\\\_   \\\\_    University Karlsruhe, Germany                      |\\n|        \\\\_     \\\\_\\\\_\\\\_     email:                                            |\\n|         \\\\_     \\\\_   \\\\_    - S_Braun@iravcl.ira.uka.de                      |\\n|          \\\\_     \\\\_\\\\_\\\\_     - UKAY@dkauni2.bitnet                           |\\n+----------------------------------------------------------------------------+\\n                         \\n\",\n",
       "  0.18352737356726545),\n",
       " ('From: hdsteven@solitude.Stanford.EDU (H. D. Stevens)\\nSubject: Re: Inflatable Mile-Long Space Billboards (was Re: Vandalizing the sky.)\\nOrganization: stanford\\nLines: 38\\n\\nIn article <YAMAUCHI.93Apr21131325@yuggoth.ces.cwru.edu>, yamauchi@ces.cwru.edu (Brian Yamauchi) writes:\\n|> >NASA would provide contractual launch services. However,\\n|> >since NASA bases its charge on seriously flawed cost estimates\\n|> >(WN 26 Mar 93) the taxpayers would bear most of the expense. This\\n|> >may look like environmental vandalism, but Mike Lawson, CEO of\\n|> >Space Marketing, told us yesterday that the real purpose of the\\n|> >project is to help the environment! The platform will carry ozone\\n|> >monitors he explained--advertising is just to help defray costs.\\n|> \\n|> This may be the purpose for the University of Colorado people.  My\\n|> guess is that the purpose for the Livermore people is to learn how to\\n|> build large, inflatable space structures.\\n|> \\n\\nThe CU people have been, and continue to be big ozone scientists. So \\nthis is consistent. It is also consistent with the new \"Comercial \\napplications\" that NASA and Clinton are pushing so hard. \\n|> \\n|> >Is NASA really supporting this junk?\\n\\nDid anyone catch the rocket that was launched with a movie advert \\nall over it? I think the rocket people got alot of $$ for painting \\nup the sides with the movie stuff. What about the Coke/Pepsi thing \\na few years back? NASA has been trying to find ways to get other \\npeople into the space funding business for some time. Frankly, I\\'ve \\nthought about trying it too. When the funding gets tight, only the \\ninnovative get funded. One of the things NASA is big on is co-funding. \\nIf a PI can show co-funding for any proposal, that proposal has a SIGNIFICANTLY\\nhigher probability of being funded than a proposal with more merit but no \\nco-funding. Once again, money talks!\\n\\n\\n-- \\nH.D. Stevens\\nStanford University\\t\\t\\tEmail:hdsteven@sun-valley.stanford.edu\\nAerospace Robotics Laboratory\\t\\tPhone:\\t(415) 725-3293  (Lab)\\nDurand Building\\t\\t\\t\\t\\t(415) 722-3296  (Bullpen)\\nStanford, CA 94305\\t\\t\\tFax:\\t(415) 725-3377\\n',\n",
       "  0.006265327663523922)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = corpus.search(\"University Karlsruhe spot from organization\")\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
